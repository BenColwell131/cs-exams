# Concurrent Systems 2014

## 2a)
The inner four loops can all be parrellised to run on multiple threads. However the bottom two loops must wait for the top two loops to complete. The four inner loops could bee vectorised and therefore perform multiple operations at once. Due to the data dependancies (Second two loops).

## 2b)

### I) Out of Order Superscalar
Superscalars are a form of Instruction Level Parallelism (ILP). The processsor executes intructions in an order governed by input data. This requires sophistcated hardware to spot and execute the parallelism.

As the superscalar cannot resturcture the code htere is no way for it to eliminate data dependancies. However as they are few and far between the architecture would have few problems parallelising this code.

### II) VLIW
Very Lone Instruction Words are another type of ILP. In this case one long instruction executes the work of N smaller instructions.

VLIW requires complex compilers to spot [] instructions and schedule them together. Such a compiler would have the cabaility to restructure the code and could work well with this program.

### III) Vector Processors
Vector Processors are quite different from the previously discussed architectures. Vector Processors work on arrays (vectors) of data at a single time. This requires the data to be local in order to get the most [] enhancements.

The code could be re-written by the programmer to run on a vecor machine. Multiple values of the innner loops could be calculated simultaneously greatly reducing costs.

### IV) Multithreaded / Simultaneous Multithreaded
Multi-threaded architectures use multiple threas to execute code in parallel. Threads can work on differnt sections withouth interference. In fine-grained MT, each thread is switched on a clock cycle. In course grained each thread is switched on a stall and finally on SMT multiple threads mix instructions.

The [] benefit of Threading with regard to this code is that threads can be switched when there is a stall due to cache missss on large arrays. As it is assumed a large array, course grained MT could be useful in this case.

Finding and exploiting parallelism is very much down to the programer in multi-threaded systems.

### V) Shared Memory Multicore Processor

In this architecture htere is a shared memory between the cores with each core having its own cache. The bus is used for cache coherency.

For this architecture to be suitable for the cache, the data must be divided into specific chunks each core can work with. In this case that would be possible. Each core could work on a portion of the data. Either the code would need to structured or an intelligent compiler must be able to [] it up.

Their could be problems regarding cache coherency.

### VI) Multi-Chip Symmetric Multiprocessor
For very large arrays this architecture is good as each core will have its own memory and bus thus greatly reducing the traffic. However, communicaion between cores can be expensive and may outweigh the benefits of parallelism.

in the above code, if the array was very large, the work could be split between the cores and the cost of communication could be cut if done correctly.

Once again the threading is up to the programmer and debugging such an architecture can be extremely difficult.

### VII) NUMA Multi-Processor
Non-Uniform Memory Access architectures have a single address space with the physical memory distributed over the chip. This means that certain areas are cheap for processors to access while other are expensive.

As every process will be accessing the same arrays this could become very costly. Each process would eventually need to make potentially many costly accesses.

### VIII) Distributed Memory Multicomputer
In this architecture there is no shared memory and messages are passed from one processor to the next. Therefore intricate and efficient message passing software would need to be written in order to make use of this architecture's full potential.

This is quite an overhead for the limited [] realistically this architecture was not designed for something so trivial and would not be a good fit.
