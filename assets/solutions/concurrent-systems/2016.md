# Concurrent Systems 2016

## 3b)

### I) Out of Order Superscalar
Out of order superscalar is a form of Instruction. [] level parallelism. The processor executes instructions [] governed by the availability of input data. It uses sophisticated hardware to spot and execute parallel [] instructions.

The data dependancy in the code would limit the processor's ability to [] this in a parrallel.

### II) Very Long Instruction Word
The idea behind VLIW is htat one instruction can complete the work of N smaller instructions. In this case the compiler is responsible for spotting the instruction level parallelism.

As the compiler for a VLIW is more complex than that of a superscalar, it may be able ot restructure the sum and create a few [] reductions to be computed in parrallel, and [] the loop.

### III) Vector Processor
Vector Processors are quite different to the two architecutres discussed above. Instead of one value being computed at any time, vectors (arrays) of values are computed simultaneously. As the code is working with an array this will be useful for vectorizing. However, as there is a reduction, the use of horizontal instructions (to sum up all the values in the final array), which are costly, must be taken into account.

Instead, an option would be to make sum a vector. This way, each lane could [] an individual sum which could be totalled together. Vectors work on structured data which is how this data is layed out.

### IV) Multithreaded / Simultaneous Multithreaded
A Multithreaded Architecture runs numerous different threads which it switches between rapidly. Thus when one thread stalls another can take up the gap improving throughput. Fine grain switches every cycle, course grain switches on stalls where as SMT mixes instructions from multiple threads.

A benefit of multithreading in relation to large arrays as per the question is that when there is a stall due to a cache miss another thread can pick up the work until the stall is finished.

The responsibility to identify and exploit parallelism is mostly down to the programmer. Through libraries sucha as P_threads and OpenMP.

### V) Shared Memory Multicore Processor
In this architecure each core has its own copy of the resources. This is great for TLP (thread-level-parallelism) as each thread can work independantly. Problems can arise however with cache coherency.

All this boils down to the cache needing to be divided among the different threads of execution. This can be acchieved on the above code. There would need to be multiple sums added together at the end.

### VI) Multiple Chip Symmetric Multiprocessor
In this architecture there is a physically centralised memory. There is a logically shared address space. Each chip will have a separate memory by []

For very large arrays the individual memory buses will be useful. However, communication between the chips can be expensive and the costs may outweigh the benefirs of the parallelism. However in this case there is no communication and all array entries can be processed independently.

### VII) NUMA
A NUMA processor has a single shared address space, however the memory is physically divided into different regions. Thus certain areas are cheap for processors to access while others are expensive. 

In this case the entire array would be accessed by all threads making the process expensive for certain threads.

### VIII) Distributed Memory Multicomputer
A distributed memory multicomputer has no shared memory and messages are passed from one processor to the next. Threfore to make this code run efficiently on the architecture, an advanced and efficient message passer would be required.

The overheads of this would most likely outweigh the costs saved and this would not be a good use of a []
